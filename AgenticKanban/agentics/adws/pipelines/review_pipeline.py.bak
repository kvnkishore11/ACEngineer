#!/usr/bin/env python3
"""
Review Pipeline for ADW System
Handles the review stage of workflow execution
"""

import json
import sys
import argparse
from pathlib import Path
from datetime import datetime
import logging


def setup_logging(task_dir: Path):
    """Setup logging for this pipeline"""
    log_file = task_dir / "review_pipeline.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


def load_task_data(task_dir: Path) -> dict:
    """Load task data from task directory"""
    task_file = task_dir / "task_data.json"
    with open(task_file, 'r') as f:
        return json.load(f)


def load_previous_stage_data(task_dir: Path) -> dict:
    """Load data from previous stages"""
    data = {}

    # Load plan data
    plan_file = task_dir / "plan_output.json"
    if plan_file.exists():
        with open(plan_file, 'r') as f:
            data['plan'] = json.load(f)

    # Load implementation data
    impl_file = task_dir / "implement_output.json"
    if impl_file.exists():
        with open(impl_file, 'r') as f:
            data['implementation'] = json.load(f)

    # Load test data
    test_file = task_dir / "test_output.json"
    if test_file.exists():
        with open(test_file, 'r') as f:
            data['testing'] = json.load(f)

    return data


def execute_review_stage(task_data: dict, stage_data: dict, task_dir: Path, logger) -> bool:
    """Execute the review stage"""
    try:
        logger.info(f"Starting review stage for task: {task_data['adw_id']}")

        # Perform different types of reviews
        review_results = {
            "code_review": perform_code_review(stage_data, task_dir, logger),
            "architecture_review": perform_architecture_review(stage_data, task_dir, logger),
            "security_review": perform_security_review(stage_data, task_dir, logger),
            "documentation_review": perform_documentation_review(stage_data, task_dir, logger),
            "compliance_review": perform_compliance_review(stage_data, task_dir, logger)
        }

        # Generate comprehensive review report
        review_report = generate_review_report(review_results, stage_data, logger)

        # Save review output
        review_output = {
            "stage": "review",
            "status": "completed" if review_report['approved'] else "needs_changes",
            "timestamp": datetime.now().isoformat(),
            "task_id": task_data['adw_id'],
            "review_results": review_results,
            "review_report": review_report,
            "approval_status": review_report['approval_status'],
            "action_items": review_report.get('action_items', [])
        }

        output_file = task_dir / "review_output.json"
        with open(output_file, 'w') as f:
            json.dump(review_output, f, indent=2)

        if review_report['approved']:
            logger.info("Review stage completed successfully - Changes approved")
            return True
        else:
            logger.warning("Review stage completed - Changes need revision")
            return True  # Still considered successful, just needs changes

    except Exception as e:
        logger.error(f"Review stage failed: {e}")
        return False


def perform_code_review(stage_data: dict, task_dir: Path, logger) -> dict:
    """Perform automated code review"""
    logger.info("Performing code review...")

    code_review = {
        "status": "completed",
        "overall_rating": "good",
        "areas_reviewed": [
            "Code quality",
            "Best practices",
            "Performance",
            "Maintainability",
            "Error handling"
        ],
        "findings": []
    }

    # Check implementation quality
    impl_data = stage_data.get('implementation', {})
    if impl_data:
        files_modified = impl_data.get('files_modified', [])

        # Simulate code review findings
        if len(files_modified) > 10:
            code_review['findings'].append({
                "type": "concern",
                "category": "scope",
                "description": "Large number of files modified - consider breaking into smaller changes",
                "severity": "medium"
            })

        # Check for specific patterns
        for file_path in files_modified:
            if 'test' not in file_path and file_path.endswith('.js'):
                code_review['findings'].append({
                    "type": "suggestion",
                    "category": "testing",
                    "description": f"Consider adding unit tests for {file_path}",
                    "severity": "low"
                })

    # Check test results
    test_data = stage_data.get('testing', {})
    if test_data:
        test_report = test_data.get('test_report', {})
        coverage = test_report.get('coverage', {}).get('line_coverage', 0)

        if coverage < 80:
            code_review['findings'].append({
                "type": "concern",
                "category": "testing",
                "description": f"Test coverage is {coverage}% - should be at least 80%",
                "severity": "high"
            })

        if test_report.get('quality_score', 100) < 80:
            code_review['findings'].append({
                "type": "concern",
                "category": "quality",
                "description": "Code quality score is below acceptable threshold",
                "severity": "medium"
            })

    # Add positive findings
    if not code_review['findings']:
        code_review['findings'].append({
            "type": "approval",
            "category": "quality",
            "description": "Code meets all quality standards",
            "severity": "info"
        })

    return code_review


def perform_architecture_review(stage_data: dict, task_dir: Path, logger) -> dict:
    """Perform architecture review"""
    logger.info("Performing architecture review...")

    arch_review = {
        "status": "completed",
        "overall_rating": "acceptable",
        "areas_reviewed": [
            "Design patterns",
            "Code organization",
            "Dependencies",
            "Scalability",
            "Integration patterns"
        ],
        "findings": []
    }

    # Check plan data for architecture decisions
    plan_data = stage_data.get('plan', {})
    if plan_data:
        plan_details = plan_data.get('plan', {})
        dependencies = plan_details.get('dependencies', [])

        if len(dependencies) > 5:
            arch_review['findings'].append({
                "type": "concern",
                "category": "complexity",
                "description": "High number of dependencies may increase system complexity",
                "severity": "medium"
            })

        # Check for architectural patterns
        if any('api' in dep.lower() for dep in dependencies):
            arch_review['findings'].append({
                "type": "observation",
                "category": "integration",
                "description": "API integration detected - ensure proper error handling and rate limiting",
                "severity": "low"
            })

    # Check implementation structure
    impl_data = stage_data.get('implementation', {})
    if impl_data:
        components_created = impl_data.get('components_created', [])

        if 'UI component' in components_created and 'API endpoint' in components_created:
            arch_review['findings'].append({
                "type": "approval",
                "category": "separation",
                "description": "Good separation of concerns between UI and API layers",
                "severity": "info"
            })

    return arch_review


def perform_security_review(stage_data: dict, task_dir: Path, logger) -> dict:
    """Perform security review"""
    logger.info("Performing security review...")

    security_review = {
        "status": "completed",
        "overall_rating": "secure",
        "areas_reviewed": [
            "Authentication",
            "Authorization",
            "Input validation",
            "Data exposure",
            "Dependency vulnerabilities"
        ],
        "findings": []
    }

    # Check test security results
    test_data = stage_data.get('testing', {})
    if test_data:
        security_results = test_data.get('test_results', {}).get('security_scan', {})
        vulnerabilities = security_results.get('vulnerabilities', {})

        high_vulns = vulnerabilities.get('high', 0)
        medium_vulns = vulnerabilities.get('medium', 0)

        if high_vulns > 0:
            security_review['findings'].append({
                "type": "blocker",
                "category": "vulnerability",
                "description": f"{high_vulns} high-severity vulnerabilities found - must be fixed",
                "severity": "critical"
            })
            security_review['overall_rating'] = "at_risk"

        if medium_vulns > 0:
            security_review['findings'].append({
                "type": "concern",
                "category": "vulnerability",
                "description": f"{medium_vulns} medium-severity vulnerabilities found - should be addressed",
                "severity": "medium"
            })

    # Add general security considerations
    impl_data = stage_data.get('implementation', {})
    if impl_data:
        files_modified = impl_data.get('files_modified', [])

        if any('auth' in file.lower() for file in files_modified):
            security_review['findings'].append({
                "type": "observation",
                "category": "authentication",
                "description": "Authentication-related changes detected - ensure secure implementation",
                "severity": "medium"
            })

        if any('api' in file.lower() for file in files_modified):
            security_review['findings'].append({
                "type": "observation",
                "category": "api_security",
                "description": "API changes detected - verify input validation and rate limiting",
                "severity": "medium"
            })

    return security_review


def perform_documentation_review(stage_data: dict, task_dir: Path, logger) -> dict:
    """Perform documentation review"""
    logger.info("Performing documentation review...")

    doc_review = {
        "status": "completed",
        "overall_rating": "adequate",
        "areas_reviewed": [
            "Code comments",
            "API documentation",
            "README updates",
            "Inline documentation",
            "Change documentation"
        ],
        "findings": []
    }

    # Check if documentation was updated
    impl_data = stage_data.get('implementation', {})
    if impl_data:
        doc_updated = impl_data.get('documentation_updated', False)

        if not doc_updated:
            doc_review['findings'].append({
                "type": "suggestion",
                "category": "documentation",
                "description": "Consider updating documentation to reflect changes",
                "severity": "low"
            })

        # Check for API changes
        files_modified = impl_data.get('files_modified', [])
        api_files = [f for f in files_modified if 'api' in f.lower() or 'endpoint' in f.lower()]

        if api_files and not doc_updated:
            doc_review['findings'].append({
                "type": "concern",
                "category": "api_docs",
                "description": "API changes detected but documentation not updated",
                "severity": "medium"
            })

    return doc_review


def perform_compliance_review(stage_data: dict, task_dir: Path, logger) -> dict:
    """Perform compliance review"""
    logger.info("Performing compliance review...")

    compliance_review = {
        "status": "completed",
        "overall_rating": "compliant",
        "areas_reviewed": [
            "Coding standards",
            "Testing requirements",
            "Change management",
            "Quality gates",
            "Process adherence"
        ],
        "findings": []
    }

    # Check if all required stages were completed
    required_outputs = ['plan', 'implementation', 'testing']
    missing_stages = [stage for stage in required_outputs if stage not in stage_data]

    if missing_stages:
        compliance_review['findings'].append({
            "type": "violation",
            "category": "process",
            "description": f"Missing required stage outputs: {', '.join(missing_stages)}",
            "severity": "high"
        })

    # Check testing compliance
    test_data = stage_data.get('testing', {})
    if test_data:
        test_report = test_data.get('test_report', {})
        coverage = test_report.get('coverage', {}).get('line_coverage', 0)

        if coverage < 70:  # Minimum compliance threshold
            compliance_review['findings'].append({
                "type": "violation",
                "category": "testing",
                "description": f"Test coverage {coverage}% below minimum requirement (70%)",
                "severity": "high"
            })

    return compliance_review


def generate_review_report(review_results: dict, stage_data: dict, logger) -> dict:
    """Generate comprehensive review report"""
    logger.info("Generating review report...")

    # Count findings by severity
    all_findings = []
    for review_type, results in review_results.items():
        findings = results.get('findings', [])
        for finding in findings:
            finding['review_type'] = review_type
            all_findings.append(finding)

    # Categorize findings
    blockers = [f for f in all_findings if f['severity'] == 'critical' or f['type'] == 'blocker']
    concerns = [f for f in all_findings if f['severity'] in ['high', 'medium'] and f['type'] != 'blocker']
    suggestions = [f for f in all_findings if f['severity'] == 'low' or f['type'] == 'suggestion']

    # Determine approval status
    approved = len(blockers) == 0
    approval_status = "approved" if approved else "changes_required"

    if len(blockers) > 0:
        approval_status = "blocked"
    elif len(concerns) > 3:
        approval_status = "conditional"

    # Generate action items
    action_items = []
    for finding in blockers + concerns:
        action_items.append({
            "priority": "high" if finding['severity'] in ['critical', 'high'] else "medium",
            "description": finding['description'],
            "category": finding['category'],
            "review_type": finding['review_type']
        })

    # Calculate overall score
    overall_score = calculate_review_score(review_results, all_findings)

    report = {
        "approved": approved,
        "approval_status": approval_status,
        "overall_score": overall_score,
        "summary": {
            "total_findings": len(all_findings),
            "blockers": len(blockers),
            "concerns": len(concerns),
            "suggestions": len(suggestions)
        },
        "findings_by_category": categorize_findings_by_type(all_findings),
        "action_items": action_items,
        "recommendations": generate_review_recommendations(review_results, all_findings),
        "sign_off": {
            "automated_review": True,
            "timestamp": datetime.now().isoformat(),
            "next_steps": "Manual review" if not approved else "Ready for deployment"
        }
    }

    return report


def categorize_findings_by_type(findings: list) -> dict:
    """Categorize findings by type"""
    categories = {}
    for finding in findings:
        category = finding['category']
        if category not in categories:
            categories[category] = []
        categories[category].append(finding)
    return categories


def calculate_review_score(review_results: dict, findings: list) -> float:
    """Calculate overall review score"""
    base_score = 100.0

    # Deduct points for findings
    for finding in findings:
        if finding['severity'] == 'critical':
            base_score -= 25
        elif finding['severity'] == 'high':
            base_score -= 15
        elif finding['severity'] == 'medium':
            base_score -= 8
        elif finding['severity'] == 'low':
            base_score -= 3

    return max(0, base_score)


def generate_review_recommendations(review_results: dict, findings: list) -> list:
    """Generate recommendations based on review results"""
    recommendations = []

    critical_findings = [f for f in findings if f['severity'] == 'critical']
    if critical_findings:
        recommendations.append("Address critical security vulnerabilities before proceeding")

    high_findings = [f for f in findings if f['severity'] == 'high']
    if len(high_findings) > 2:
        recommendations.append("Multiple high-priority issues found - prioritize fixes")

    # Check specific categories
    security_findings = [f for f in findings if f['category'] in ['vulnerability', 'security']]
    if security_findings:
        recommendations.append("Review security implications and implement additional safeguards")

    testing_findings = [f for f in findings if f['category'] == 'testing']
    if testing_findings:
        recommendations.append("Improve test coverage and quality")

    if not findings:
        recommendations.append("Excellent work! All review criteria met.")

    return recommendations


def main():
    parser = argparse.ArgumentParser(description="Review Pipeline")
    parser.add_argument("--task-id", required=True, help="Task ID")
    parser.add_argument("--task-dir", required=True, type=Path, help="Task directory")
    parser.add_argument("--stage", required=True, help="Stage name")

    args = parser.parse_args()

    # Setup logging
    logger = setup_logging(args.task_dir)

    try:
        # Load task data
        task_data = load_task_data(args.task_dir)
        stage_data = load_previous_stage_data(args.task_dir)

        # Execute review stage
        success = execute_review_stage(task_data, stage_data, args.task_dir, logger)

        if success:
            logger.info("Review pipeline completed successfully")
            sys.exit(0)
        else:
            logger.error("Review pipeline failed")
            sys.exit(1)

    except Exception as e:
        logger.error(f"Review pipeline error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()